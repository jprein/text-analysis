---
title: "read_PDF_folder"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown
```{r}
# packages
library(here) # for current working directory
library(pdftools)
library(tidyverse)
library(tidytext)
library(dplyr)
library(SnowballC) # for word stemming

# saves the names of all the pdf documents which are in your current folder
file_list <- list.files(here(), pattern = "*.pdf")
# creates a list with as many entries as you have pdf files. in here, we will save all the text content from the pdf files
data_list <- vector("list", "length" = length(file_list))

# for every pdf file, read in the text and save it in data list
for (i in seq_along(file_list)) {
   data_list[[i]] <- pdf_text(file_list[i]) %>% 
    readr::read_lines()
}

# load existing, predefined list of stopwords (words that we don't want to count)
# you could also create a customized list of words to exclude
data("stop_words")

word <- c("al", "age", "study", "studies", "studied", "time", "task", "tasks", "relation", "relationship", "relative", "relatively", "related", "relating", "relates", "relate", "score", "scores", "scored", "scoring", "measure", "measures", "measurement","month", "months", "ability", "abilities", "correlation", "correlate", "correlates", "correlations", "significance", "significant", "test", "tests", "skill", "skills", "research", "differ", "differs", "different", "difference", "differences", "variable", "variables", "https", "token", "type", "e.g", "doi", "journal", "level", "levels", "")
custom_stop_words <- tibble(word)

# create function to count words
# arguments:
# - original_text: all the text content
# - pdfname: an ID of the current text, so name your pdfs meaningfully!
count_words <- function(original_text, pdfname){
  # get rid of .pdf suffix
  pdfname = str_remove(pdfname, ".pdf")
  # save text as tibble
  original_text <- tibble(original_text)
  # text preprocessing
  tidy_text <- original_text %>%
    # select the (one and only) column that contains all of our text 
    select(original_text) %>%
    # split strings into words (automatically converts all letters to lowercase & removes punctuation)
    unnest_tokens("word", original_text) %>%
    # returns all rows that are not included in our stopwords
    anti_join(stop_words, by = "word") %>%
    anti_join(custom_stop_words, by = "word")
  
  # remove all numeric digits (minus in front of grep tells it to exclude rather then include them)
  tidy_text <- tidy_text[-grep("\\b\\d+\\b", tidy_text$word), ]
  # remove white spaces
  tidy_text$word <- gsub("\\s+", "", tidy_text$word)
  
  # count word frequencies
  word_frequencies <- tidy_text %>%
    # stemming: replacing word with its most basic conjugate form
    # TODO: mutate_at superseded but this doesn't work:   
    # mutate(across("word", funs(wordStem((.), language = "en")))) %>%
    mutate_at("word", funs(wordStem((.), language = "en"))) %>%
    # well... counts words!
    count(word) %>%
    # order according to highest value
    arrange(desc(n)) %>%
    # name column according to the second argument. use bang bang to unquote the left hand side of the assignment
    rename(!!pdfname := "n")
  
  return(word_frequencies)
}

# create list of tibbles to save words and their frequencies
analyzed_data_list <- vector("list", "length" = length(file_list))

# for every pdf that we read in, count word frequencies
for (i in seq_along(file_list)) {
  analyzed_data_list[[i]] <- count_words(original_text = data_list[[i]], pdfname = file_list[i])
}

# merge all the tibbles from our list together
analyzed_tibble <- analyzed_data_list %>%
  reduce(full_join, by = "word")

# calculate total word frequency across pds
analyzed_tibble$sum <- analyzed_tibble %>%
  select(-word) %>%
  rowSums(na.rm = TRUE)

# only have a look at the summary
word_freq_sum <- analyzed_tibble %>%
  select(word, sum)
word_freq_sum

# save word frequencies according to pdfs (study acts as ID)
word_freq_per_pdf <- analyzed_tibble %>%
  select(-sum) %>%
  gather(study, count, -word) %>%
  group_by(word) %>%
  select(study, word, count)

# show most frequent words across corpus
plot_freq <- analyzed_tibble %>%
  select(-sum) %>%
  gather(study, count, -word) %>%
  group_by(word) %>%
  select(study, word, count)%>%
  summarise(sum = sum(count, na.rm = T))%>%
  arrange(-sum) %>%
  #  rotation of 90 degrees for a random subset of 40 % of the words
  mutate(angle = 90 * sample(c(0, 1), n(), replace = TRUE, prob = c(60, 40)))

# VISUALIZATION
library(ggwordcloud)
# by default, ggplot uses square root scaling (but does not map 0 to 0)
ggplot(head(plot_freq, 50), aes(label = word, size = sum, angle = angle)) +
  # geom_text_wordcloud_area(): scale the font of each label so that the text area is a function of the raw size aesthetic (when used in combination with scale_size_area)
  # area_corr_power: By default, the area is proportional to the raw size aesthetic raised to the power 1/.7 in order to match the human area perception. To obtain an area proportional to the raw size aesthetic, it suffices to set the area_corr_power to 1
  # eccentricity: by default a vertical eccentricity of .65, so that the spiral is 1/.65 wider than taller.
  geom_text_wordcloud_area(area_corr_power = 1, rm_outside = T, eccentricity = 1) + 
  # for true proportionality
  scale_size_area(max_size = 12) +
  theme_minimal()

# TODO: combine child, children & children'
# if(word_freq_sum$word == c("child", "children", "children\'")) 
  
```

