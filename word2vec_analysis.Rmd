---
title: "word2vec_analysis"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r load_packages, message=FALSE, warning=FALSE}
library(here) # for current working directory
library(pdftools)
library(tidyverse)
library(tidytext)
library(dplyr)
library(SnowballC) # for word stemming
library(udpipe) # tokenising etc. 
library(word2vec)
library(uwot) # dimensionality reduction
library(ggrepel) # for geom_text_repel()
```

```{r load_data}
# saves the names of all the pdf documents which are in your current folder
file_list <- list.files(here(), pattern = "*.pdf")
raw_text <- c("")

# attach all the text to each other
for (i in seq_along(file_list)) {
  current_text <- pdf_text(file_list[i]) %>% 
    readr::read_lines()
  raw_text <- c(raw_text, current_text)
}
rm(current_text)

```

```{r text_preprocessing}
# load existing, predefined list of stopwords (words that we don't want to count)
# you could also create a customized list of words to exclude
data("stop_words")

word <- c("al", "age", "study", "studies", "studied", "time", "task", "tasks", "relation", "relationship", "relative", "relatively", "related", "relating", "relates", "relate", "score", "scores", "scored", "scoring", "measure", "measures", "measurement","month", "months", "ability", "abilities", "correlation", "correlate", "correlates", "correlations", "significance", "significant", "test", "tests", "skill", "skills", "research", "differ", "differs", "different", "difference", "differences", "variable", "variables", "https", "token", "type", "e.g", "doi", "journal", "level", "levels", "")
custom_stop_words <- tibble(word)
rm(word)

# text preprocessing 
tidy_text <- tibble(raw_text)

tidy_text <- tidy_text %>%
  # select the (one and only) column that contains all of our text 
  select(raw_text) %>%
  # split strings into words (automatically converts all letters to lowercase & removes punctuation)
  unnest_tokens("word", raw_text) %>%
  # returns all rows that are not included in our stopwords
  anti_join(stop_words, by = "word") %>%
  anti_join(custom_stop_words, by = "word")

# remove all numeric digits
# \S* zero or more non-whitespace characters
# \d+ one ore more digits
tidy_text <- tidy_text[-grep("\\S*\\d+\\S*", tidy_text$word), ]

# remove white spaces
# tidy_text$word <- gsub("\\s+", "", tidy_text$word)

# stemming: replacing word with its most basic conjugate form
tidy_text <- tidy_text %>%
  # TODO: mutate_at & funs superseded but this doesn't work:   
  # mutate(across("word", funs(wordStem((.), language = "en")))) %>%
  mutate_at("word", funs(wordStem((.), language = "en"))) 
```

```{r count_words}
# somehow wordstemming doesn't work for child, children, children' 
# therefore, do word stemming manually, and then add to tibble
child_n <- tidy_text %>%
  filter(str_detect(word, "child")) %>%
  count(word) %>%
  select(n) %>%
  sum()

# count words (with manually done wordstemming for "child")
word_freq <- tidy_text %>%
  count(word) %>%
  filter(!str_detect(word, "child")) %>%
  add_row(word = "child", n = child_n) %>%
  arrange(-n)
rm(child_n)

# save top 25 most frequent words
top25 <- c()
for (i in 1:25) {
  new_word <- summarise(word_freq[i,], text = str_c(word, collapse = ""))
  top25 <- c(top25, new_word$text)
}
# same for top50
top50 <- c()
for (i in 1:50) {
  new_word <- summarise(word_freq[i,], text = str_c(word, collapse = ""))
  top50 <- c(top50, new_word$text)
}

# same for top100
top100 <- c()
for (i in 1:100) {
  new_word <- summarise(word_freq[i,], text = str_c(word, collapse = ""))
  top100 <- c(top100, new_word$text)
}
rm(new_word)
```

```{r recreate_context}
# TODO: how to define context? 20 words? => change by value
# create empty character vector in which we save our single strings 
tidy_chr_vec <- c()
for (i in seq(from = 1, to = length(tidy_text$word), by = 20)) {
  
  # if our number of words is divisible by 20, then just do everything as usual
  # meaning: take the next 20 words and put them together as a string. combine with what we already have
  if(length(tidy_text$word) %% 20 == 0) {
    new_string <- summarise(tidy_text[i:(i+19), ], text = str_c(word, collapse = " "))
    tidy_chr_vec <- c(tidy_chr_vec, new_string$text)
    
    # if word number is not divisible by 20, then...
  } else {
    
    # if we are not at the last iteration, do everything as usual
    if(i < max(seq(from = 1, to = length(tidy_text$word), by = 20))) {
      new_string <- summarise(tidy_text[i:(i + 19), ], text = str_c(word, collapse = " "))
      tidy_chr_vec <- c(tidy_chr_vec, new_string$text)
      
      # for the last iteration, there are not 20 words left to bind together. 
      # therefore, with modulo, find out how many words are still left. glue those together to our strings.
    } else {
      new_string <- summarise(tidy_text[i:(i + ((length(tidy_text$word) %% 20) - 1)), ], text = str_c(word, collapse = " "))
      tidy_chr_vec <- c(tidy_chr_vec, new_string$text)
    }
  }
}
rm(new_string)
```

```{r word2vec_model}
word2vec_model <- word2vec(x = tidy_chr_vec, type = "cbow", dim = 15, iter = 20)

# once you have a model, you can get the embeddings of all words using as.matrix, which has for our case 15 columns as we specified dim = 15
embedding_all <- as.matrix(word2vec_model)
# if you want to analyze only a certain set of words
# embedding <- predict(w2v_model, c("bus", "impeccable"), type = "embedding")
head(embedding_all)

lookslike_all <- predict(word2vec_model, c("social", "cognit", "mind", "belief"), type = "nearest", top_n = 10)
lookslike_all
```

```{r visualize_2D}
# 2D plot by semantic similarity
# dimensionality reduction using UMAP (maps embeddings in 2D)
# n_neighbors: size of local neighborhood (in terms of number of neighboring sample points) used for manifold approximation. Larger values result in more global views of the manifold, while smaller values result in more local data being preserved. In general values should be in the range 2 to 100.
viz <- umap(embedding_all, n_neighbors = 15, n_threads = 2)
rownames(viz) <- rownames(embedding_all)

allwords_2D <- data.frame(word = gsub("//.+", "", rownames(viz)), x = viz[, 1], y = viz[, 2], stringsAsFactors = FALSE)
rm(viz)
# if you want to manually choose which words to display
# custom_2D <- allwords_2D %>%
#   filter(word %in% c("social", "cognition", "belief", "desire", "knowledge", "perception", "goal", "reasoning", "thinking",  "emotion", "aggression", "mind", "awareness", "play", "pretense", "peer", "family", "parents", "mother", "father", "child", "children", "understand", "language", "tom", "predict", "prediction", "action", "attention", "attend", "interact", "interaction", "prosocial", "school"))

# for top 25
top25_2D <- allwords_2D %>%
  filter(word %in% top25)

# semantic proximity (as is)
ggplot(top25_2D, aes(x = x, y = y, label = word)) +
  geom_text_repel() + theme_void()

# semantic proximity + word frequency
top25_freq <- left_join(x = top25_2D, y = word_freq, by = "word")
ggplot(top25_freq, aes(x = x, y = y, label = word, size = log(n))) +
  geom_text_repel() + theme_void()

# for top 50
top50_2D <- allwords_2D %>%
  filter(word %in% top50)

# semantic proximity (as is)
ggplot(top50_2D, aes(x = x, y = y, label = word)) +
  geom_text_repel() + theme_void()

# semantic proximity + word frequency
top50_freq <- left_join(x = top50_2D, y = word_freq, by = "word")
ggplot(top50_freq, aes(x = x, y = y, label = word, size = log(n))) +
  geom_text_repel() + theme_void()

# for top 100
top100_2D <- allwords_2D %>%
  filter(word %in% top100)

# semantic proximity (as is)
ggplot(top100_2D, aes(x = x, y = y, label = word)) +
  geom_text_repel() + theme_void()

# semantic proximity + word frequency
top100_freq <- left_join(x = top100_2D, y = word_freq, by = "word")
ggplot(top100_freq, aes(x = x, y = y, label = word, size = log(n))) +
  geom_text_repel() + theme_void()

```

```{r old_code_notes}
# data(brussels_reviews)
# x <- subset(brussels_reviews, language == "fr")
# x <- tolower(x$feedback)
# cat(x[1])

# we can write embeddings to disk and read them back in (not necessary for us)

# training parameters
# dim: dimensionality of the word vectors. usually more is better, but not always, especially with small data
# type: skip-gram (slower, better for infrequent words) vs cbow(fast)
# window: for skip_gram usually around 10, for cbow around 5
# hs: the training algorithm: hierarchical softmax (better for infrequent words) vs negative sampling (better for frequent words, better with low dimensional vectors)
# sample: subsampling of frequent words: can improve both accuracy and speed for large data sets /useful values are in range 0.001 to 0.00001)


# # TRY TO DO TEXT PREPROCESSING WITHOUT TIBBLE
# cleaned_raw_text <- raw_text %>%
#   # get rid of capital letters
#   tolower() %>%
#   # replace all words that contain numbers with empty string
#   gsub("\\S*\\d+\\S*", "", .) 

# only take first word in pattern
  #%>%
  # gsub(stop_words$word, "", .) %>%
  # gsub(pattern = word, replacement = "", x = ., fixed = T)

# wordStem doesn't work, since long string per row, not single tokens/words

# FINDING DENOMINATORS OF GIVEN NUMBER: gmp::factorize(length(tidy_text$word))
  
```

