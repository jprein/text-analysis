---
title: "word2vec_analysis"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r load_packages, message=FALSE, warning=FALSE}
library(here) # for current working directory
library(pdftools)
library(tidyverse)
library(tidytext)
library(dplyr)
library(SnowballC) # for word stemming
library(udpipe) # tokenising etc. 
library(word2vec)
library(uwot) # dimensionality reduction
library(ggrepel) # for geom_text_repel()
```

```{r load_data}
# saves the names of all the pdf documents which are in your current folder
file_list <- list.files(here(), pattern = "*.pdf")
all_text <- c("")

# attach all the text to each other
for (i in seq_along(file_list)) {
  current_text <- pdf_text(file_list[i]) %>% 
    readr::read_lines()
  all_text <- c(all_text, current_text)
}
```

```{r text_preprocessing}
# load existing, predefined list of stopwords (words that we don't want to count)
# you could also create a customized list of words to exclude
data("stop_words")

word <- c("al", "age", "study", "studies", "studied", "time", "task", "tasks", "relation", "relationship", "relative", "relatively", "related", "relating", "relates", "relate", "score", "scores", "scored", "scoring", "measure", "measures", "measurement","month", "months", "ability", "abilities", "correlation", "correlate", "correlates", "correlations", "significance", "significant", "test", "tests", "skill", "skills", "research", "differ", "differs", "different", "difference", "differences", "variable", "variables", "https", "token", "type", "e.g", "doi", "journal", "level", "levels", "")
custom_stop_words <- tibble(word)

# text preprocessing 
tidy_text <- tibble(all_text)

tidy_text <- tidy_text %>%
  # select the (one and only) column that contains all of our text 
  select(all_text) %>%
  # split strings into words (automatically converts all letters to lowercase & removes punctuation)
  unnest_tokens("word", all_text) %>%
  # returns all rows that are not included in our stopwords
  anti_join(stop_words, by = "word") %>%
  anti_join(custom_stop_words, by = "word")

# remove all numeric digits
# \S* zero or more non-whitespace characters
# \d+ one ore more digits
tidy_text <- tidy_text[-grep("\\S*\\d+\\S*", tidy_text$word), ]

# remove white spaces
#tidy_text$word <- gsub("\\s+", "", tidy_text$word)

# stemming: replacing word with its most basic conjugate form
tidy_text <- tidy_text %>%
  # TODO: mutate_at & funs superseded but this doesn't work:   
  # mutate(across("word", funs(wordStem((.), language = "en")))) %>%
  mutate_at("word", funs(wordStem((.), language = "en"))) 
```

```{r count_words}
tidy_text %>%
  count(word) %>%
  arrange(-n)
```
```{r unnest_tibble}
# text_string <- tidy_text[1:1000, 1] %>%
#  summarise(text = str_c(word, collapse = " "))
# 
# text_string
# tstring <- text_string$text

# not necessary, just use tidy_text$word
```

```{r notes}
# data(brussels_reviews)
# x <- subset(brussels_reviews, language == "fr")
# x <- tolower(x$feedback)
# cat(x[1])

# we can write embeddings to disk and read them back in (not necessary for us)

# training parameters
# dim: dimensionality of the word vectors. usually more is better, but not always, especially with small data
# type: skip-gram (slower, better for infrequent words) vs cbow(fast)
# window: for skip_gram usually around 10, for cbow around 5
# hs: the training algorithm: hierarchical softmax (better for infrequent words) vs negative sampling (better for frequent words, better with low dimensional vectors)
# sample: subsampling of frequent words: can improve both accuracy and speed for large data sets /useful values are in range 0.001 to 0.00001)

# PROBLEM:
# text preprocessing zerschießt zeilen und damit context
# brauchen wir text preprocessing überhaupt? beim model suchen wir ja eh nach den relevanten wörtern
# und fürs wörter zählen können wir immer noch preprocessen
# hab vorher ausprobiert alles in eine zeile zu packen, das geht aber auch nicht mit modeln (also chunk eins drüber)
```

```{r}
# TRY TO DO TEXT PREPROCESSING WITHOUT TIBBLE
head(all_text, 20) %>%
  # get rid of capital letters
  tolower() %>%
  # replace all words that contain numbers with empty string
  gsub("\\S*\\d+\\S*", "", .) 

# only take first word in pattern
  #%>%
  # gsub(stop_words$word, "", .) %>%
  # gsub(pattern = word, replacement = "", x = ., fixed = T)

# wordStem doesn't work, since long string per row, not single tokens/words
  
```


```{r word2vec_untidy}
# train word2vec model by passing on txt or .txt file
w2v_model_all <- word2vec(x = all_text, type = "cbow", dim = 15, iter = 20)

# once you have a model, you can get the embeddings of all words using as.matrix, which has for our case 15 columns as we specified dim = 15
embedding_all <- as.matrix(w2v_model_all)
# if you want to analyze only a certain set of words
# embedding <- predict(w2v_model, c("bus", "impeccable"), type = "embedding")
head(embedding_all)

lookslike_all <- predict(w2v_model_all, c("social", "cognition", "mind", "cognitive"), type = "nearest", top_n = 5)
lookslike_all
```

```{r word2vec_tidy}
# THIS PART DOES (TECHNICALLY) WORK BUT RESULTS ARE WEIRD
# BY USING TIBBLE TO CLEAN; WE LOOSE CONTEXT OF WORDS
# train word2vec model by passing on txt or .txt file
w2v_model_tidy <- word2vec(x = tidy_text$word, type = "cbow", dim = 15, iter = 20)

# once you have a model, you can get the embeddings of all words using as.matrix, which has for our case 15 columns as we specified dim = 15
embedding_tidy <- as.matrix(w2v_model_tidy)
# if you want to analyze only a certain set of words
# embedding <- predict(w2v_model, c("bus", "impeccable"), type = "embedding")
head(embedding_tidy)

lookslike_tidy <- predict(w2v_model_tidy, c("social", "cognit", "mind"), type = "nearest", top_n = 5)
lookslike_tidy
```

```{r}
# DOESNT WORK
# # train word2vec model by passing on txt or .txt file
# w2v_model_tidy_1 <- word2vec(x = tstring, type = "cbow", dim = 15, iter = 20)
# 
# # once you have a model, you can get the embeddings of all words using as.matrix, which has for our case 15 columns as we specified dim = 15
# embedding_tidy_1 <- as.matrix(w2v_model_tidy_1)
# # if you want to analyze only a certain set of words
# # embedding <- predict(w2v_model, c("bus", "impeccable"), type = "embedding")
# head(embedding_tidy_1)
# 
# lookslike_tidy_1 <- predict(w2v_model_tidy_1, c("mind"), type = "nearest", top_n = 5)
# lookslike_tidy_1
```

```{r visualize_2D}
# 2D plot by semantic similarity
# dimensionality reduction using UMAP (maps embeddings in 2D)
# n_neighbors: size of local neighborhood (in terms of number of neighboring sample points) used for manifold approximation. Larger values result in more global views of the manifold, while smaller values result in more local data being preserved. In general values should be in the range 2 to 100.
viz <- umap(embedding_all, n_neighbors = 15, n_threads = 2)
rownames(viz) <- rownames(embedding_all)
head(viz, n = 10)

df <- data.frame(word = gsub("//.+", "", rownames(viz)), 
                 upos = gsub(".+//", "", rownames(viz)),
                 x = viz[, 1], y = viz[, 2],
                 stringsAsFactors = FALSE)

# choose which words you want to display. can't represent all, too many words
df_subset <- df %>%
  filter(word %in% c("social", "cognition", "belief", "desire", "knowledge", "perception", "goal", "reasoning", "thinking",  "emotion", "aggression", "mind", "awareness", "play", "pretense", "peer", "family", "parents", "mother", "father", "child", "children", "understand", "language", "tom", "predict", "prediction", "action", "attention", "attend", "interact", "interaction", "prosocial", "ignorance", "school"))

ggplot(df_subset, aes(x = x, y = y, label = word)) +
  geom_text_repel() + theme_void()

```

